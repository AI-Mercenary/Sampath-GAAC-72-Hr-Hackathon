# Install required libraries
# We use these libraries to handle deep learning (PyTorch, transformers), video/audio processing (yt-dlp, moviepy, OpenCV), 
# and various utilities (pillow, reportlab, etc.)
!pip install torch torchvision torchaudio transformers opencv-python-headless moviepy pillow reportlab yt-dlp google-api-python-client nbformat nbconvert psutil

# Import necessary modules
# yt_dlp is used to download videos/audio from YouTube
# torch is used for deep learning tasks
# librosa is used for audio processing
# transformers provides pre-trained models for tasks like transcription and summarization
# os is used for file and directory operations
# psutil is used to monitor memory usage during processing
import yt_dlp
import torch
import librosa
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline
import os
import psutil

# Function to download audio from a YouTube link
# We use yt_dlp to fetch audio from a given video and convert it to WAV format
def download_audio_from_youtube(link, output_path):
    # Configure yt-dlp for audio extraction
    ydl_opts = {
        'format': 'bestaudio/best',  # We choose the best available audio quality
        'outtmpl': output_path,  # Output file will be saved at this path
        'quiet': True,  # Suppress detailed download logs
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',  # We use FFmpeg to extract audio
            'preferredcodec': 'wav',  # The audio is saved in WAV format
            'preferredquality': '192',  # We set the quality for the extracted audio
        }],
    }
    # Download and save the audio
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([link])
    print(f"Audio downloaded and saved at {output_path}")

# Function to monitor memory usage
# We use psutil to track memory consumption during execution
def print_memory_usage():
    process = psutil.Process(os.getpid())  # Get the current process ID
    memory = process.memory_info().rss / 1024 ** 2  # Convert memory usage to MB
    print(f"Memory usage: {memory:.2f} MB")

# Function to transcribe audio using Wav2Vec2
# Wav2Vec2 is a pre-trained speech-to-text model available in HuggingFace's transformers library
def transcribe_audio_with_huggingface(audio_path, batch_size=30):
    print_memory_usage()  # Display memory usage before model loading
    print("Loading the Wav2Vec2 model for transcription...")
    # Load the pre-trained processor and model
    processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
    model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
    print_memory_usage()  # Display memory usage after model loading

    print("Processing audio for transcription...")
    # Load the audio file and resample it to 16,000 Hz (required for the model)
    audio, rate = librosa.load(audio_path, sr=16000)
    total_length = len(audio)  # Get the total length of the audio in samples
    transcription = ""  # Initialize an empty string to store the transcription

    # Split the audio into smaller batches for processing
    for i in range(0, total_length, batch_size * rate):
        # Extract a batch of audio samples
        batch_audio = audio[i:i + batch_size * rate]
        if len(batch_audio) == 0:  # Stop if the batch is empty
            break
        # Convert the batch into input tensors for the model
        input_values = processor(batch_audio, sampling_rate=rate, return_tensors="pt").input_values
        print_memory_usage()  # Display memory usage during batch processing

        print("Transcribing the current batch of audio...")
        # Get the model's predictions (logits) for the audio batch
        logits = model(input_values).logits
        # Convert the logits into predicted token IDs
        predicted_ids = torch.argmax(logits, dim=-1)
        # Check if any predicted IDs are invalid
        if predicted_ids.max() >= model.config.vocab_size:
            print(f"Warning: Invalid predicted ID {predicted_ids.max()} detected.")
            continue
        # Decode the predicted token IDs into text
        batch_transcription = processor.decode(predicted_ids[0])
        transcription += batch_transcription + " "  # Append the transcription of this batch

        # Clear temporary variables to free memory
        del batch_audio, input_values, logits, predicted_ids
        torch.cuda.empty_cache()  # Explicitly clear GPU memory
        print_memory_usage()  # Display memory usage after processing the batch

    return transcription.strip()  # Return the complete transcription

# Function to save the transcription into a text file
# We save the transcription as a report for further processing
def generate_text_file_from_transcription(transcription, output_text_path):
    print("Generating a text file for the transcription...")
    with open(output_text_path, 'w') as file:  # Open the file in write mode
        file.write("Audio Transcription Report\n\n")
        file.write(transcription)  # Write the transcription content
    print(f"Transcription saved at {output_text_path}")

# Function to extract video metadata from YouTube
# We use yt_dlp to fetch the video's title and description without downloading it
def extract_video_metadata(link):
    ydl_opts = {'quiet': True}  # Suppress yt-dlp logs
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(link, download=False)  # Extract metadata only
    title = info.get("title", "No title available")  # Extract the video title
    description = info.get("description", "No description available")  # Extract the video description
    return title, description

# Function to read the transcription from a file
# This allows us to load and process the transcription later
def read_transcription_file(transcription_path):
    with open(transcription_path, 'r') as file:  # Open the file in read mode
        transcription = file.read()  # Read the content of the file
    return transcription  # Return the transcription text

# Function to summarize a text chunk
# We use a pre-trained summarization model (BART) from HuggingFace
def summarize_text_chunk(chunk, title, description):
    # Initialize the summarizer pipeline with BART
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    # Combine the title, description, and text chunk for context
    text = "Title: " + title + " Description: " + description + " transcript chunk: " + chunk
    # Generate a summary of the input text
    summary = summarizer(text, max_length=150, min_length=50, do_sample=False)
    return summary[0]['summary_text']  # Return the generated summary

# Function to save the summary into a text file
# We write the summary as bullet points in a report
def generate_text_file_from_summary(summary, output_text_path):
    print("Generating a summary report...")
    with open(output_text_path, 'w') as file:  # Open the file in write mode
        file.write("Summary Report\n\n")
        points = summary.split('. ')  # Split the summary into points
        for point in points:
            file.write(f"- {point.strip()}.\n")  # Write each point as a bullet
    print(f"Summary report saved at {output_text_path}")

# Main function to process the video
# We perform audio download, transcription, and summarization in sequence
def process_video(link, audio_path, audio_file_path_creation, text_path, summary_text_path):
    download_audio_from_youtube(link, audio_file_path_creation)  # Step 1: Download audio
    transcription = transcribe_audio_with_huggingface(audio_path)  # Step 2: Transcribe audio
    generate_text_file_from_transcription(transcription, text_path)  # Step 3: Save transcription
    title, description = extract_video_metadata(link)  # Step 4: Extract metadata
    transcription = read_transcription_file(text_path)  # Step 5: Read transcription
    chunks = [transcription[i:i + 1000] for i in range(0, len(transcription), 1000)]  # Split text into chunks
    final_summary = ""  # Initialize summary
    for chunk in chunks:
        summary = summarize_text_chunk(chunk, title, description)  # Summarize each chunk
        final_summary += summary + " "
    final_summary = summarize_text_chunk(final_summary, title, description)  # Final summary
    generate_text_file_from_summary(final_summary.strip(), summary_text_path)  # Save summary

# Specify video link and file paths
# We define paths for storing the processed files
video_link = "https://www.youtube.com/watch?v=0oGJTQCy4cQ"
audio_file_path = "./processed/audio_extracted.wav"
audio_file_path_creation = "./processed/audio_extracted"
text_file_path = "./processed/audio_transcription.txt"
summary_text_path = "./processed/summary_report.txt"
os.makedirs("./processed", exist_ok=True)  # Create the directory if it doesn't exist

# Call the main function to process the video
process_video(video_link, audio_file_path, audio_file_path_creation, text_file_path, summary_text_path)
